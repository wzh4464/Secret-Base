---
toc: true
documentclass: "ctexart"
classoption: "UTF8"
zotero-key: EP2FS5DJ
zt-attachments:
  - "15307"
title: Stochastic delayed adversarial imitation learning
citekey: xie2024StochasticDelayedAdversarial
---
# Stochastic delayed adversarial imitation learning

[Zotero](zotero://select/library/items/EP2FS5DJ) [attachment](file:///Volumes/Mac_Ext/Zotero/storage/RGWXRUN5/SMC24_0429_MS.pdf)

> [!note] Page 1
> 
> Abstract—
> 
> ---
> 实时竞价（RTB）已广泛应用于在线展示广告中，广告主为每个展示机会竞价以展示他们的广告。为了优化广告主的广告效果，RTB中提出了大量的竞价策略，其中大多数旨在通过竞价参数来推导出最佳竞价函数。主流的竞价策略之一是基于实时反馈调整竞价参数，利用强化学习（RL）。然而，RL方法在实际应用中通常会遇到诸如随机延迟等挑战，导致RL竞价策略表现不佳。本文提出了一种名为高效探索的逆向经验重放（BEE）的新框架，该框架利用随机延迟对抗模仿学习（SDAIL）来应对这些挑战。BEE由两个模块组成：辅助探索策略和优先级采样技术。辅助探索策略引入辅助策略，以实现RL训练过程中的高效探索。优先级采样技术结合了两种新的采样方法：逆向优先经验重放（BPER）和关键区域优先经验重放（KPER），协同实现RL模型的有效更新和快速收敛。我们在xiaohongshu.com的工业数据集上进行了大量实验，结果表明，BEE在收敛速度和广告效果方面优于先前的方法，展示了BEE方法的有效性。
> ^NWTT4BVJaRGWXRUN5p1

> [!note] Page 1
> 
> INTRODUCTION
> 
> ---
> 在移动社交媒体时代，在线展示广告是新兴广告形式中最重要的一种。据互联网广告局（IAB）报告显示，2022年美国的在线展示广告收入比2021年增长了10.8%，总计2097亿美元[1]。实时竞价（RTB）[2]是一种广泛采用的在线展示广告范式，广告主为每次展示机会竞价，展示机会随后分配给出价最高的广告主。随着RTB的发展，出现了许多竞价策略，通过最大化转化率在广告主指定的预算和关键绩效指标（KPI）约束下提高广告效果。传统上，大多数竞价策略研究将竞价问题表述为线性规划问题，旨在通过竞价参数推导出最佳竞价函数。在主流方法中，一种方法是通过强化学习（RL）根据实时反馈调整竞价参数。RL是一种著名的范式，在实际任务中取得了显著成功[3]，[4]，[5]，并已在RTB中实现了竞价参数的调整。一些基于RL的方法，如USCB[6]、SoRL[7]和CBRL[8]，已被开发以解决RTB中稀疏奖励和在线与离线环境之间不一致的问题。然而，基于RL的方法存在一些限制，包括低效的探索和缓慢的收敛，导致表现不佳甚至在RTB中失败。为了解决RL方法在RTB中面临的挑战，我们提出了一种创新的基于RL的算法，名为高效探索的逆向经验重放（BEE），优化了收集数据的利用率。意识到RL中的数据利用包括环境中的探索和从重放缓冲区中采样经验[9]，BEE结合了两个模块：辅助探索策略和优先级采样技术。这些模块重新定义了RL如何利用数据，提升了广告效果。具体来说，辅助探索策略在早期学习中被引入，以改善探索并加速训练。在训练的初始阶段，智能体通常随机与环境交互，导致累积大量劣质经验，阻碍学习过程。为了解决这一挑战，我们引入了一种辅助策略，可以是基于规则的方法或简单预训练的网络，在决策时间歇性地替代智能体的策略。通过这样做，我们能够更高效地探索特征空间。借助辅助探索策略的优势，我们能够收集到更多高质量的经验，从而促进增强学习。在优先级采样技术的部署中，我们提出了两种新的采样方法，以更好地利用存储的数据，实现更快的收敛。根据RL中的贝尔曼方程，经验的Q值取决于其后继的Q值。如果采样策略忽略了后续经验Q值估计的精度，可能会导致Q网络的错误更新。为了应对这个问题，逆向经验重放（RER）[10]被提出作为解决方案。RER通过逆序采样经验并执行更新来解决问题。然而，这种方法重新引入了经验之间的时间相关性，这是一种对神经网络性能有害的数据特征。因此，我们将RER与优先级经验重放（PER）结合，提出了一种新的采样策略，名为逆向优先经验重放（BPER）。BPER对轨迹后段的经验赋予高重要性，导致Q网络的更快收敛。此外，RTB中的高维特征空间在高效采样方面存在挑战，因其高维性、大量信息和特征分布不均。这些因素使得快速获取有价值的经验变得困难，从而增加了算法的训练难度。为了解决这一问题，我们提出了关键区域优先经验重放（KPER），将特征空间划分为关键区域，在这些区域内的经验被高优先级采样。借助KPER，智能体能够更高效地学习关键区域的有意义经验，从而在更少的更新中取得更好的表现。为了证明我们提出的BEE算法的有效性，我们在xiaohongshu.com的大规模工业数据集上进行了评估。实验结果表明，BEE在收敛速度和广告表现方面优于先前的方法。我们总结了本文的主要贡献如下：我们提出了BEE算法，在满足曝光和利润率约束的同时最大化转化率。我们设计了两个新模块，辅助探索策略和优先级采样技术，这些模块促进了高维特征空间中的高效训练，使得精确学习和快速收敛成为可能，从而提升广告表现。我们在xiaohongshu.com的实际工业数据集上进行了实证评估，展示了BEE算法相对于先前方法的优越性。
> ^Z22XFZSLaRGWXRUN5p1

> [!note] Page 2
> 
> Backward Prioritized Experience Replay
> 
> ---
> Seems not novel
> ^HWJ37REFaRGWXRUN5p2

> [!note] Page 2
> 
> partition
> 
> ---
> How to partition
> ^B37HEA6WaRGWXRUN5p2

> [!note] Page 2
> 
> RELATED WORKS
> 
> ---
> 基于强化学习的竞价：已经提出了几种成功的基于RL的方法用于RTB。在[13]中，竞价问题被表述为一个马尔可夫决策过程（MDP），通过调整展示级别的竞价价格进行在线决策。[14]提出了一种基于RL的方法，直接确定最终竞价，提高了RTB环境中的鲁棒性。在[15]中，设计了一个奖励网络，以学习合适的奖励，克服误导性即时奖励的问题。USCB[6]将约束竞价抽象为一个顺序参数调整问题，通过奖励塑形决定竞价参数。为了应对在线和离线环境之间的不一致，提出了SoRL[7]。SoRL设计了一种探索策略与在线环境交互，并通过方差抑制方法训练竞价代理。[8]基于贝叶斯RL提出了CBRL，以解决非静态广告市场中的ROI约束竞价问题。
> ^BV76IXBEaRGWXRUN5p2

> [!note] Page 2
> 
> Reinforcement Learning-based Bidding
> 
> ---
> not complete
> ^HLE9TMSDaRGWXRUN5p2

> [!note] Page 8
> 
> Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015
> 
> ---
> ICLR 2016
> ^8SZZVPMRaRGWXRUN5p8
